
This is a C++ implementation of a linguistic transformer.  It is far more
memory efficient than Python (200 MB versus 8G).  

The names of many files are currently hardcoded as this module is currently
under active development.  A language model requires two ancillary
object, they are a vocabulary of tokens and a set of initial vector
embeddings for the tokens.  They are found in the Data folder. The
Data folder contains 3 files:

SherlockHolmesNormalized.txt - generated with Tools/normalizeText.py

The normalized file is used to build a dictionary of tokens and their
initial vector embeddings.

Sherlock.E - the initial embeddings generated with Tools/vocab_generate.py

Both the causal and the masked example use the same file.

causal and masked accept the same command line options:

-i maximum number of epochs to run
-r specify the random number generator seed: invaluable for debugging etc.
-n the number of transformer blocks
-h the number of heads
-N the number of exemplars per epoch (need control for use with notebook computers)

There are two models demonstrating two of the more important LLM families,
causal (used for generative AI) and masked.  The masked model implements one
of BERT's training tasks, masked tokens.

Positional encodings are currently turned off as they are not compatible 
with the current initial embeddings (normalized and drowned out).

